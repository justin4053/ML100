{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [教學目標]\n",
    "學習 sklearn 中，各種評估指標的使用與意義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [範例重點]\n",
    "注意觀察各指標的數值範圍，以及輸入函數中的資料格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回歸問題\n",
    "常見的評估指標有\n",
    "- MAE\n",
    "- MSE\n",
    "- R-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們隨機生成(X, y)資料，然後使用線性回歸模型做預測，再使用 MAE, MSE, R-square 評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  2.841797252565566\n",
      "MSE:  12.48868006739824\n",
      "R-square:  0.9916581036260311\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.make_regression(n_features=1, random_state=42, noise=4) # 生成資料\n",
    "model = LinearRegression() # 建立回歸模型\n",
    "model.fit(X, y) # 將資料放進模型訓練\n",
    "prediction = model.predict(X) # 進行預測\n",
    "mae = metrics.mean_absolute_error(prediction, y) # 使用 MAE 評估\n",
    "mse = metrics.mean_squared_error(prediction, y) # 使用 MSE 評估\n",
    "r2 = metrics.r2_score(prediction, y) # 使用 r-square 評估\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"R-square: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類問題\n",
    "常見的評估指標有\n",
    "- AUC\n",
    "- F1-Score (Precision, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer() # 我們使用 sklearn 內含的乳癌資料集\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test) # 測試集中的 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.random.random((50,)) # 我們先隨機生成 50 筆預測值，範圍都在 0~1 之間，代表機率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34710663 0.90849653 0.93124869 0.64323056 0.83270707 0.71620915\n",
      " 0.48692821 0.60788655 0.17789154 0.50061279 0.94772468 0.72333423\n",
      " 0.35675434 0.88479792 0.70322486 0.31000397 0.95373681 0.43603908\n",
      " 0.99676135 0.43401971 0.4255163  0.7286116  0.15378613 0.60961554\n",
      " 0.25176709 0.5809835  0.31544607 0.7048369  0.54060422 0.29838287\n",
      " 0.20861449 0.00327288 0.79928533 0.48864107 0.08898133 0.92340656\n",
      " 0.99367932 0.36457182 0.28771887 0.80196172 0.1081142  0.30349314\n",
      " 0.83719029 0.86553005 0.19515694 0.05533782 0.78946384 0.87455202\n",
      " 0.01778147 0.93714081]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.5704584040747028\n"
     ]
    }
   ],
   "source": [
    "auc = metrics.roc_auc_score(y_test, y_pred) # 使用 roc_auc_score 來評估。 **這邊特別注意 y_pred 必須要放機率值進去!**\n",
    "print(\"AUC: \", auc) # 得到結果約 0.5，與亂猜的結果相近，因為我們的預測值是用隨機生成的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score:  0.689655172413793\n",
      "Precision:  0.7407407407407407\n",
      "Recall:  0.6451612903225806\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5 \n",
    "y_pred_binarized = np.where(y_pred>threshold, 1, 0) # 使用 np.where 函數, 將 y_pred > 0.5 的值變為 1，小於 0.5 的為 0\n",
    "f1 = metrics.f1_score(y_test, y_pred_binarized) # 使用 F1-Score 評估\n",
    "precision = metrics.precision_score(y_test, y_pred_binarized) # 使用 Precision 評估\n",
    "recall  = metrics.recall_score(y_test, y_pred_binarized) # 使用 recall 評估\n",
    "print(\"F1-Score: \", f1) \n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "了解 F1-score 的公式意義，並試著理解程式碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "請參考 F1-score 的公式與[原始碼](https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/metrics/classification.py#L620)，試著寫出 F2-Score 的計算函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',sample_weight=None):\n",
    "    \"\"\"Compute the F1 score, also known as balanced F-score or F-measure\n",
    "    The F1 score can be interpreted as a weighted average of the precision and\n",
    "    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "    The relative contribution of precision and recall to the F1 score are\n",
    "    equal. The formula for the F1 score is::\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    In the multi-class and multi-label case, this is the average of\n",
    "    the F1 score of each class with weighting depending on the ``average``\n",
    "    parameter.\n",
    "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "        Estimated targets as returned by a classifier.\n",
    "    labels : list, optional\n",
    "        The set of labels to include when ``average != 'binary'``, and their\n",
    "        order if ``average is None``. Labels present in the data can be\n",
    "        excluded, for example to calculate a multiclass average ignoring a\n",
    "        majority negative class, while labels not present in the data will\n",
    "        result in 0 components in a macro average. For multilabel targets,\n",
    "        labels are column indices. By default, all labels in ``y_true`` and\n",
    "        ``y_pred`` are used in sorted order.\n",
    "        .. versionchanged:: 0.17\n",
    "           parameter *labels* improved for multiclass problem.\n",
    "    pos_label : str or int, 1 by default\n",
    "        The class to report if ``average='binary'`` and the data is binary.\n",
    "        If the data are multiclass or multilabel, this will be ignored;\n",
    "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
    "        scores for that label only.\n",
    "    average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \\\n",
    "                       'weighted']\n",
    "        This parameter is required for multiclass/multilabel targets.\n",
    "        If ``None``, the scores for each class are returned. Otherwise, this\n",
    "        determines the type of averaging performed on the data:\n",
    "        ``'binary'``:\n",
    "            Only report results for the class specified by ``pos_label``.\n",
    "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "        ``'micro'``:\n",
    "            Calculate metrics globally by counting the total true positives,\n",
    "            false negatives and false positives.\n",
    "        ``'macro'``:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "            Calculate metrics for each label, and find their average weighted\n",
    "            by support (the number of true instances for each label). This\n",
    "            alters 'macro' to account for label imbalance; it can result in an\n",
    "            F-score that is not between precision and recall.\n",
    "        ``'samples'``:\n",
    "            Calculate metrics for each instance, and find their average (only\n",
    "            meaningful for multilabel classification where this differs from\n",
    "            :func:`accuracy_score`).\n",
    "    sample_weight : array-like of shape = [n_samples], optional\n",
    "        Sample weights.\n",
    "    Returns\n",
    "    -------\n",
    "    f1_score : float or array of float, shape = [n_unique_labels]\n",
    "        F1 score of the positive class in binary classification or weighted\n",
    "        average of the F1 scores of each class for the multiclass task.\n",
    "    References\n",
    "    ----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
